{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing with Apache Spark\n",
    "### Student ID: [#####]\n",
    "### Subtasks Done: [#,#,..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'python3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Goals:\n",
    "   1. Quick overview of Apache Spark\n",
    "   3. Work with hands-on examples so as to get familiar with Apach Spark.\n",
    "  \n",
    "## Lab Tasks:\n",
    "   1. Building and submiting the first Spark application (word count). (0 point)\n",
    "   2. Modify the previous example to filter out less frequent words and submit the task into the Spark.  (0.5 point)\n",
    "   3. Modify task 1 so as to return the longest *line*.  (0.5 point)\n",
    "   4. Given a data set of movie ratings by users, calcualte the average movie rate  (1 point)\n",
    "   5. (Bonus task) List of articles requested by user.  (1 point)\n",
    "  \n",
    "   \n",
    "Next week, we are going to process streaming data using Apache Spark Structured Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark:\n",
    "    Spark is an open source engine for large-scale data processing. Spark applications can be written by Scala, Python or java. However, it provides spark-shell for learning purposes.\n",
    " <img src=\"https://spark.apache.org/images/spark-logo-trademark.png\">\n",
    "\n",
    "### Spark Basics:\n",
    "    The fundamental data unit in Spark is RDD (resilient distributed dataset). The Spark programming is mainly performing operations on RDD. RDDs can be created from different sources such as data in memory, files etc...\n",
    "    There are, mainly, two types of operations on Spark RDD:\n",
    "    1. Transformations: generate a new RDD based on the existing one. For example:\n",
    "        -map(func): forms a new RDD by passing each element of the source through a function “func”.\n",
    "        -filter(func): forms a new RDD by selecting  elements of the source on which “func” returns true.\n",
    "        -flatMap(func): similar to map, but each input item can be mapped to 0 or more output items (so “func” should return a sequence rather than a single item).\n",
    "    2. Actions: return values. For Example:\n",
    "        -count(): counts the number of elements in the RDD.\n",
    "        -collect(): returns all elements in an array\n",
    "        \n",
    "     Any Spark application starts from the entry point. The entry point to the spark APIs is spearkContext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Shell:\n",
    " Spark Shell is an interactive shell that you can open by typing \"$ pyspark\" (in the case of using python with spark) in your terminal. You will then see in your terminal like in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "## Task 1: Building and submitting the first Spark application (words count):\n",
    "    In this example we are going to read the text in a given file and count the number of words in the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"Spark Lab words count Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "conf.setMaster('spark://spark-master:7077')\n",
    "conf.set('spark.driver.bindAddress', '0.0.0.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read data in the text file. Every line will be a value in  the RDD. Then split each line into space-separated words such that each word will be a value in the RDD.\n",
    "\n",
    "\n",
    "    For example if it is an RDD of a single entry \"Hello world\", then the result will be two entries:\n",
    "    Hello\n",
    "    World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Download AliceInWonderLandPart1.txt from canvas and put it in ```/home/bitnami/Lab2``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = sc.textFile(\"data/AliceInWonderLandPart1.txt\").flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts = tokenized.map(lambda word: (word, 1)).reduceByKey(lambda v1,v2:v1 +v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = wordCounts.collect()\n",
    "print ('[%s]' % ', '.join(map(str, list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark submission\n",
    "\n",
    "To submit this Spark job, you need to assembly lines above into a python file, Let it be wordCount.py. \n",
    "   - You can check the wordCount.py attached with lab as an example\n",
    "   -  Submit the job using the following command $spark-submit WordCount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Modify the previous example to filter out words that occur less than N times and submit the task into the Spark.:\n",
    "The previous example is to count the words in a given file.\n",
    "\n",
    "in this task, you will:<br>\n",
    "    1- Assemble the code in the previous task and put it in ```/home/bitnami/Lab2/Data/FliterLessFrequentWords.py``` <br>\n",
    "    2- Modify the example so as to filter out the words that occur less than a given value N. Right before converting the RDD to a list, you will write your code.<br>\n",
    "    3- N will be a parameter supplied with your job (hint: countThreshold=int(sys.argv[1])) <br> \n",
    "    4- Now, the job has to be submitted Spark. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li> You should submit </li>\n",
    "    <ul>\n",
    "        <li> the .py file with your code </li>\n",
    "        <li> put the screenshot for spark-submit in the next cell or do the spark-submit inside the notebook</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Screenshot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Modify task #1 so as to return the longest *line* :\n",
    "In this task you will modify the task 1 as well. However, instead of counting the words you will return the longest line among the input lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li> You should submit </li>\n",
    "    <ul>\n",
    "        <li> the .py file with your code </li>\n",
    "        <li> put the screenshot for spark-submit in the next cell or do the spark-submit inside the notebook</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#screenshot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Average Movie Rate\n",
    "<a href=https://grouplens.org/datasets/movielens/> In this website </a>, you can find several datasets for movies ratings\n",
    "   1. Navigate to MovieLens 100K Dataset\n",
    "   2. Download the file ml-100k.zip\n",
    "   3. unzip this and navigate to the file u.data\n",
    "   4. the format of the data is as the following example \n",
    "196    242    3    881250949. \n",
    "the first number indicates userID; the second is movieID; the third is the rate which the user gave to this movie and it ranges from 1 to 5; and the last number indicates the timestamp.\n",
    "   5. Write a Spark application that computes the average rating per movie. So the answer will be for example:\n",
    "\n",
    "    1 3.0\n",
    "    \n",
    "    2 2.4\n",
    "  \n",
    "   and so on\n",
    "\n",
    "Such that the first number represents the movie and the second number represents the average rating across the data set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li> You should submit </li>\n",
    "    <ul>\n",
    "        <li> the .py file with your code </li>\n",
    "        <li> put the screenshot for spark-submit in the next cell or do the spark-submit inside the notebook</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#screenshot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 Bonus: List of articles requested by users:\n",
    "  <p>\n",
    "    In this task we need to get a list of articles requested by each user as the following:\n",
    "   <ul> \n",
    "       <li> Please find the attached two files with this session (weblogs, kblist).</li>\n",
    "   <li> The structure of the input files and the output is presented in the figure below. The weblogs file records the IP address-UserID then the Http command, e.g. GET, issued against a resource. We are interested in knowledge base documents. They are named by KBDOC-XXXXX.html. The kblist file contains entries in the form KBDOC-XXXXX title.</li>\n",
    "   <li> The ouput should consist of pairs where the first element is the UserID and the second element is the list of knowledge base document titles requested by the user. </li>\n",
    "   </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='BonusTask.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "<ul>\n",
    "<li> Map separate datasets into key‐value Pair RDDs </li>\n",
    "    <ul>\n",
    "        <li> Map web log requests to (docid,userid) </li>\n",
    "        <li> Map KB Doc index to (docid,title)</li>\n",
    "    </ul>\n",
    "<li> Join by key: docid</li>\n",
    "<li> Map joined data into the desired format: (userid,title)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected delieverables\n",
    "\n",
    " - Nmae each .py file with the task_taskNumber\n",
    " - put all .py files and the notebook in a folder and compress it then submit it on canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
