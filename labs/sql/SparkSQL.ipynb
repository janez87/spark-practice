{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Spark Dataframes and SQL (Part III)\n",
    "Analyzing airline data with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Analyzing airline data\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring SQL query options\n",
    "\n",
    "We start by some static data by using ```sc.parallelize``` to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dataframe with different data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = sc.parallelize([Row(id = 1,\n",
    "                             name = \"Jill\",\n",
    "                             active = True,\n",
    "                             clubs = ['chess', 'hockey'],\n",
    "                             subjects = {\"math\": 80, 'english': 56},\n",
    "                             enrolled = datetime(2014, 8, 1, 14, 1, 5)),\n",
    "                         Row(id = 2,\n",
    "                             name = \"George\",\n",
    "                             active = False,\n",
    "                             clubs = ['chess', 'soccer'],\n",
    "                             subjects = {\"math\": 60, 'english': 96},\n",
    "                             enrolled = datetime(2015, 3, 21, 8, 2, 5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------------------+---+------+--------------------+\n",
      "|active|          clubs|           enrolled| id|  name|            subjects|\n",
      "+------+---------------+-------------------+---+------+--------------------+\n",
      "|  true|[chess, hockey]|2014-08-01 14:01:05|  1|  Jill|[english -> 56, m...|\n",
      "| false|[chess, soccer]|2015-03-21 08:02:05|  2|George|[english -> 96, m...|\n",
      "+------+---------------+-------------------+---+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "record_df = record.toDF()\n",
    "record_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register the dataframe as a temporary view\n",
    "\n",
    "* The view is valid for one session\n",
    "* This is required to run SQL commands on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_df.createOrReplaceTempView(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------------------+---+------+--------------------+\n",
      "|active|          clubs|           enrolled| id|  name|            subjects|\n",
      "+------+---------------+-------------------+---+------+--------------------+\n",
      "|  true|[chess, hockey]|2014-08-01 14:01:05|  1|  Jill|[english -> 56, m...|\n",
      "| false|[chess, soccer]|2015-03-21 08:02:05|  2|George|[english -> 96, m...|\n",
      "+------+---------------+-------------------+---+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_records_df = sqlContext.sql('SELECT * FROM records')\n",
    "\n",
    "all_records_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access array and dictionary (map) elements in the dataframe in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------------+\n",
      "| id|clubs[1]|subjects[english]|\n",
      "+---+--------+-----------------+\n",
      "|  1|  hockey|               56|\n",
      "|  2|  soccer|               96|\n",
      "+---+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT id, clubs[1], subjects[\"english\"] FROM records').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply expressions on columns, e.g., negate the active column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|(NOT active)|\n",
      "+---+------------+\n",
      "|  1|       false|\n",
      "|  2|        true|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT id, NOT active FROM records').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional statements in SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------------------+---+----+--------------------+\n",
      "|active|          clubs|           enrolled| id|name|            subjects|\n",
      "+------+---------------+-------------------+---+----+--------------------+\n",
      "|  true|[chess, hockey]|2014-08-01 14:01:05|  1|Jill|[english -> 56, m...|\n",
      "+------+---------------+-------------------+---+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT * FROM records where active').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------------------+---+------+--------------------+\n",
      "|active|          clubs|           enrolled| id|  name|            subjects|\n",
      "+------+---------------+-------------------+---+------+--------------------+\n",
      "| false|[chess, soccer]|2015-03-21 08:02:05|  2|George|[english -> 96, m...|\n",
      "+------+---------------+-------------------+---+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT * FROM records where subjects[\"english\"] > 90').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global temporary view\n",
    "\n",
    "* Temporary view shared across multiple sessions\n",
    "* Kept alive till the Spark application terminates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_df.createGlobalTempView(\"global_records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------------------+---+------+--------------------+\n",
      "|active|          clubs|           enrolled| id|  name|            subjects|\n",
      "+------+---------------+-------------------+---+------+--------------------+\n",
      "|  true|[chess, hockey]|2014-08-01 14:01:05|  1|  Jill|[english -> 56, m...|\n",
      "| false|[chess, soccer]|2015-03-21 08:02:05|  2|George|[english -> 96, m...|\n",
      "+------+---------------+-------------------+---+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT * FROM global_temp.global_records').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's play with real data. we start creating a session and load the data. \n",
    "We have three data sets, two small and one large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Analyzing airline data\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in airline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You may need to adjust the paths below to match the exact location on your machine.\n",
    "airlinesPath = \"airlines.csv\"\n",
    "flightsPath = \"flights.csv\"\n",
    "airportsPath = \"airports.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = spark.read\\\n",
    "                .format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load(airlinesPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register as a temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.createOrReplaceTempView(\"airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Code', 'Description']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines = spark.sql(\"SELECT * FROM airlines\")\n",
    "airlines.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "| Code|         Description|\n",
      "+-----+--------------------+\n",
      "|19031|Mackey Internatio...|\n",
      "|19032|Munz Northern Air...|\n",
      "|19033|Cochise Airlines ...|\n",
      "|19034|Golden Gate Airli...|\n",
      "|19035|  Aeromech Inc.: RZZ|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load flights data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = spark.read\\\n",
    "               .format(\"csv\")\\\n",
    "               .option(\"header\", \"true\")\\\n",
    "               .load(flightsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'airlines',\n",
       " 'flight_number',\n",
       " 'origin',\n",
       " 'destination',\n",
       " 'departure',\n",
       " 'departure_delay',\n",
       " 'arrival',\n",
       " 'arrival_delay',\n",
       " 'air_time',\n",
       " 'distance']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.createOrReplaceTempView(\"flights\")\n",
    "\n",
    "flights.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------+------+-----------+---------+---------------+-------+-------------+--------+--------+\n",
      "|      date|airlines|flight_number|origin|destination|departure|departure_delay|arrival|arrival_delay|air_time|distance|\n",
      "+----------+--------+-------------+------+-----------+---------+---------------+-------+-------------+--------+--------+\n",
      "|2014-04-01|   19805|            1|   JFK|        LAX|     0854|          -6.00|   1217|         2.00|  355.00| 2475.00|\n",
      "|2014-04-01|   19805|            2|   LAX|        JFK|     0944|          14.00|   1736|       -29.00|  269.00| 2475.00|\n",
      "|2014-04-01|   19805|            3|   JFK|        LAX|     1224|          -6.00|   1614|        39.00|  371.00| 2475.00|\n",
      "|2014-04-01|   19805|            4|   LAX|        JFK|     1240|          25.00|   2028|       -27.00|  264.00| 2475.00|\n",
      "|2014-04-01|   19805|            5|   DFW|        HNL|     1300|          -5.00|   1650|        15.00|  510.00| 3784.00|\n",
      "+----------+--------+-------------+------+-----------+---------+---------------+-------+-------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting with dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476881, 1579)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.count(), airlines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_count = spark.sql(\"SELECT COUNT(*) FROM flights\")\n",
    "airlines_count = spark.sql(\"SELECT COUNT(*) FROM airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[count(1): bigint], DataFrame[count(1): bigint])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_count, airlines_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the result of an SQL expression is a dataframe. So, we have to call ```collect()``` to get an array and access the first row/cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476881, 1579)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_count.collect()[0][0], airlines_count.collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframes created using SQL commands can be aggregated, grouped etc. exactly as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_distance_df = spark.sql(\"SELECT distance FROM flights\")\\\n",
    "                         .agg({\"distance\":\"sum\"})\\\n",
    "                         .withColumnRenamed(\"sum(distance)\",\"total_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|total_distance|\n",
      "+--------------+\n",
      "|  3.79052917E8|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_distance_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing flight delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_delays_2012 = spark.sql(\n",
    "    \"SELECT date, airlines, flight_number, departure_delay \" +\n",
    "    \"FROM flights WHERE departure_delay > 0 and year(date) = 2012\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------------+---------------+\n",
      "|date|airlines|flight_number|departure_delay|\n",
      "+----+--------+-------------+---------------+\n",
      "+----+--------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_delays_2012.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------+---------------+\n",
      "|      date|airlines|flight_number|departure_delay|\n",
      "+----------+--------+-------------+---------------+\n",
      "|2014-04-01|   19805|            2|          14.00|\n",
      "|2014-04-01|   19805|            4|          25.00|\n",
      "|2014-04-01|   19805|            6|         126.00|\n",
      "|2014-04-01|   19805|            7|         125.00|\n",
      "|2014-04-01|   19805|            8|           4.00|\n",
      "+----------+--------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_delays_2014 = spark.sql(\n",
    "    \"SELECT date, airlines, flight_number, departure_delay \" +\n",
    "    \"FROM flights WHERE departure_delay > 0 and year(date) = 2014\")\n",
    "\n",
    "all_delays_2014.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_delays_2014.createOrReplaceTempView(\"all_delays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------+---------------+\n",
      "|      date|airlines|flight_number|departure_delay|\n",
      "+----------+--------+-------------+---------------+\n",
      "|2014-04-27|   20366|         5246|          99.00|\n",
      "|2014-04-27|   19393|         2948|          99.00|\n",
      "|2014-04-27|   20366|         5365|          99.00|\n",
      "|2014-04-26|   19977|          616|          99.00|\n",
      "|2014-04-27|   20366|         6030|          99.00|\n",
      "+----------+--------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_delays_2014.orderBy(all_delays_2014.departure_delay.desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of delayed flights in 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_count = spark.sql(\"SELECT COUNT(departure_delay) FROM all_delays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(departure_delay)|\n",
      "+----------------------+\n",
      "|                179015|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delay_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179015"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay_count.collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentage of flights delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.53871510922012"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay_percent = delay_count.collect()[0][0] / flights_count.collect()[0][0] * 100\n",
    "delay_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding delay per aIrlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_per_airline = spark.sql(\"SELECT airlines, departure_delay FROM flights\")\\\n",
    "                         .groupBy(\"airlines\")\\\n",
    "                         .agg({\"departure_delay\":\"avg\"})\\\n",
    "                         .withColumnRenamed(\"avg(departure_delay)\", \"departure_delay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|airlines|   departure_delay|\n",
      "+--------+------------------+\n",
      "|   19393|13.429567657134724|\n",
      "|   20366|12.296210112379818|\n",
      "|   19977| 8.818392620527979|\n",
      "|   20436| 8.716275167785234|\n",
      "|   20409|  8.31110357194785|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delay_per_airline.orderBy(delay_per_airline.departure_delay.desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_per_airline.createOrReplaceTempView(\"delay_per_airline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_per_airline = spark.sql(\"SELECT * FROM delay_per_airline ORDER BY departure_delay DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|airlines|   departure_delay|\n",
      "+--------+------------------+\n",
      "|   19393|13.429567657134724|\n",
      "|   20366|12.296210112379818|\n",
      "|   19977| 8.818392620527979|\n",
      "|   20436| 8.716275167785234|\n",
      "|   20409|  8.31110357194785|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delay_per_airline.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL join operations \n",
    "\n",
    "* Get the names of the delayed flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----+--------------------+\n",
      "|airlines|   departure_delay| Code|         Description|\n",
      "+--------+------------------+-----+--------------------+\n",
      "|   19393|13.429567657134724|19393|Southwest Airline...|\n",
      "|   20366|12.296210112379818|20366|ExpressJet Airlin...|\n",
      "|   19977| 8.818392620527979|19977|United Air Lines ...|\n",
      "|   20436| 8.716275167785234|20436|Frontier Airlines...|\n",
      "|   20409|  8.31110357194785|20409| JetBlue Airways: B6|\n",
      "+--------+------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delay_per_airline = spark.sql(\"SELECT * FROM delay_per_airline \" +\n",
    "                              \"JOIN airlines ON airlines.code = delay_per_airline.airlines \" +\n",
    "                              \"ORDER BY departure_delay DESC\")\n",
    "\n",
    "delay_per_airline.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+--------------------+\n",
      "|airlines|   departure_delay|         Description|\n",
      "+--------+------------------+--------------------+\n",
      "|   19393|13.429567657134724|Southwest Airline...|\n",
      "|   20366|12.296210112379818|ExpressJet Airlin...|\n",
      "|   19977| 8.818392620527979|United Air Lines ...|\n",
      "|   20436| 8.716275167785234|Frontier Airlines...|\n",
      "|   20409|  8.31110357194785| JetBlue Airways: B6|\n",
      "+--------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delay_per_airline.drop(\"code\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window functions\n",
    "\n",
    "In SQL, window functions allow making computations over a range of rows (tuples). Window functions provide a handy approach to answer analytical queries like *Find the best selling product in each category in last week's sale*. Window functions are part of the SQL standard and they are also available in Spark SQL/Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = spark.read\\\n",
    "                .format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load('products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+\n",
      "|   product|category|price|\n",
      "+----------+--------+-----+\n",
      "|Samsung TX|  Tablet|  999|\n",
      "|Samsung JX|  Mobile|  799|\n",
      "|Redmi Note|  Mobile|  399|\n",
      "|        Mi|  Mobile|  299|\n",
      "|      iPad|  Tablet|  789|\n",
      "|    iPhone|  Mobile|  999|\n",
      "|  Micromax|  Mobile|  249|\n",
      "|    Lenovo|  Tablet|  499|\n",
      "|   OnePlus|  Mobile|  356|\n",
      "|        Xu|  Tablet|  267|\n",
      "+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window rank function\n",
    "\n",
    "You can order the data in the range specified and automatically you get a ```rank``` column for each tuple. We need to specify: the column(s) to partition the data with, the column(s) to sort the data with in order to compute the rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec1 = Window \\\n",
    "    .partitionBy(products['category']) \\\n",
    "    .orderBy(products['price'].desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_rank = (func.rank().over(windowSpec1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+----+\n",
      "|   product|category|price|rank|\n",
      "+----------+--------+-----+----+\n",
      "|    iPhone|  Mobile|  999|   1|\n",
      "|Samsung JX|  Mobile|  799|   2|\n",
      "|Redmi Note|  Mobile|  399|   3|\n",
      "|   OnePlus|  Mobile|  356|   4|\n",
      "|        Mi|  Mobile|  299|   5|\n",
      "|  Micromax|  Mobile|  249|   6|\n",
      "|Samsung TX|  Tablet|  999|   1|\n",
      "|      iPad|  Tablet|  789|   2|\n",
      "|    Lenovo|  Tablet|  499|   3|\n",
      "|        Xu|  Tablet|  267|   4|\n",
      "+----------+--------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_rank = products.select(\n",
    "        products['product'],\n",
    "        products['category'],\n",
    "        products['price']\n",
    ").withColumn('rank', price_rank)\n",
    "\n",
    "product_rank.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window max function between rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec2 = Window \\\n",
    "    .partitionBy(products['category']) \\\n",
    "    .orderBy(products['price'].desc()) \\\n",
    "    .rowsBetween(-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_max = (func.max(products['price']).over(windowSpec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+---------+\n",
      "|   product|category|price|price_max|\n",
      "+----------+--------+-----+---------+\n",
      "|    iPhone|  Mobile|  999|      999|\n",
      "|Samsung JX|  Mobile|  799|      999|\n",
      "|Redmi Note|  Mobile|  399|      799|\n",
      "|   OnePlus|  Mobile|  356|      399|\n",
      "|        Mi|  Mobile|  299|      356|\n",
      "|  Micromax|  Mobile|  249|      299|\n",
      "|Samsung TX|  Tablet|  999|      999|\n",
      "|      iPad|  Tablet|  789|      999|\n",
      "|    Lenovo|  Tablet|  499|      789|\n",
      "|        Xu|  Tablet|  267|      499|\n",
      "+----------+--------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.select(\n",
    "    products['product'],\n",
    "    products['category'],\n",
    "    products['price'],\n",
    "    price_max.alias(\"price_max\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window price difference function between ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec3 = Window \\\n",
    "    .partitionBy(products['category']) \\\n",
    "    .orderBy(products['price'].desc()) \\\n",
    "    .rangeBetween(-sys.maxsize, sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_difference = \\\n",
    "  (func.max(products['price']).over(windowSpec3) - products['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+----------------+\n",
      "|   product|category|price|price_difference|\n",
      "+----------+--------+-----+----------------+\n",
      "|    iPhone|  Mobile|  999|             0.0|\n",
      "|Samsung JX|  Mobile|  799|           200.0|\n",
      "|Redmi Note|  Mobile|  399|           600.0|\n",
      "|   OnePlus|  Mobile|  356|           643.0|\n",
      "|        Mi|  Mobile|  299|           700.0|\n",
      "|  Micromax|  Mobile|  249|           750.0|\n",
      "|Samsung TX|  Tablet|  999|             0.0|\n",
      "|      iPad|  Tablet|  789|           210.0|\n",
      "|    Lenovo|  Tablet|  499|           500.0|\n",
      "|        Xu|  Tablet|  267|           732.0|\n",
      "+----------+--------+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.select(\n",
    "    products['product'],\n",
    "    products['category'],\n",
    "    products['price'],\n",
    "    price_difference.alias(\"price_difference\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec4 = Window \\\n",
    "    .partitionBy(products['category']) \\\n",
    "    .orderBy(products['price'].asc()) \\\n",
    "    .rangeBetween(0, sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372036854775807"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.maxsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_max = (func.max(products['price']).over(windowSpec4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+---------+\n",
      "|   product|category|price|price_max|\n",
      "+----------+--------+-----+---------+\n",
      "|  Micromax|  Mobile|  249|      999|\n",
      "|        Mi|  Mobile|  299|      999|\n",
      "|   OnePlus|  Mobile|  356|      999|\n",
      "|Redmi Note|  Mobile|  399|      999|\n",
      "|Samsung JX|  Mobile|  799|      999|\n",
      "|    iPhone|  Mobile|  999|      999|\n",
      "|        Xu|  Tablet|  267|      999|\n",
      "|    Lenovo|  Tablet|  499|      999|\n",
      "|      iPad|  Tablet|  789|      999|\n",
      "|Samsung TX|  Tablet|  999|      999|\n",
      "+----------+--------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.select(\n",
    "    products['product'],\n",
    "    products['category'],\n",
    "    products['price'],\n",
    "    price_max.alias(\"price_max\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
