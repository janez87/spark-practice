{"cells":[{"cell_type":"markdown","source":["### Structured Streaming\n  \n  \n* Kafka \n* Aggregations\n* Time windows\n* Watermarking\n* Joins"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8307cf20-40d9-4eef-9d0b-af50f5901b50"}}},{"cell_type":"code","source":["# for getting data from Kafka (or other distributed log systems), we need minimum 2 things:\n# the server\n# the topic\n\nfrom pyspark.sql.functions import col\n\nkafka_server = \"server1.databricks.training:9092\"   # US (Oregon)\n\nwiki_df = (spark.readStream                        # Get the DataStreamReader\n  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n  .option(\"subscribe\", \"en\")                       # Subscribe to the \"en\" Kafka topic - edits of English wikipedia pages\n  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n  .load()                                          # Load the DataFrame\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b2068d2-2855-4d9b-a6e1-2e7d57853e53"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(wiki_df, streamName = \"wiki_raw_stream\")\n\n#key - the data key. Used in state machines, not useful in this case\n#value - the data, in binary format. This is our JSON payload. We'll need to cast it to STRING.\n#topic - the topic we are subscribing to\n#partition - partition. This server only has one partition.\n#offset - the offset value. This is per topic, partition, and consumer group\n#timestamp - the timestamp\n#timestampType - whether timestamp is created time or log append time"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d215cc9f-6821-49be-b0f6-1fe2480ff07c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# let's have a look at the JSON payload\n\nfrom pyspark.sql.types import StringType\n\nwiki_payload_df = (wiki_df\n                  .select(col(\"value\").cast(StringType()))\n                  )\n\ndisplay(wiki_payload_df,streamName=\"wiki_payload_stream\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98a522a2-5e9e-4f8d-a9f2-f5db0012b652"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# let's create a schema for navigating the JSON payload\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\n\nschema = StructType([\n  StructField(\"channel\", StringType(), True),\n  StructField(\"comment\", StringType(), True),\n  StructField(\"delta\", IntegerType(), True),\n  StructField(\"flag\", StringType(), True),\n  StructField(\"geocoding\", StructType([                 # (OBJECT): Added by the server, field contains IP address geocoding information for anonymous edit.\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"countryCode2\", StringType(), True),\n    StructField(\"countryCode3\", StringType(), True),\n    StructField(\"stateProvince\", StringType(), True),\n    StructField(\"latitude\", DoubleType(), True),\n    StructField(\"longitude\", DoubleType(), True),\n  ]), True),\n  StructField(\"isAnonymous\", BooleanType(), True),      # (BOOLEAN): Whether or not the change was made by an anonymous user\n  StructField(\"isNewPage\", BooleanType(), True),\n  StructField(\"isRobot\", BooleanType(), True),\n  StructField(\"isUnpatrolled\", BooleanType(), True),\n  StructField(\"namespace\", StringType(), True),         # (STRING): Page's namespace. See https://en.wikipedia.org/wiki/Wikipedia:Namespace \n  StructField(\"page\", StringType(), True),              # (STRING): Printable name of the page that was edited\n  StructField(\"pageURL\", StringType(), True),           # (STRING): URL of the page that was edited\n  StructField(\"timestamp\", StringType(), True),         # (STRING): Time the edit occurred, in ISO-8601 format\n  StructField(\"url\", StringType(), True),\n  StructField(\"user\", StringType(), True),              # (STRING): User who made the edit or the IP address associated with the anonymous editor\n  StructField(\"userURL\", StringType(), True),\n  StructField(\"wikipediaURL\", StringType(), True),\n  StructField(\"wikipedia\", StringType(), True),         # (STRING): Short name of the Wikipedia that was edited (e.g., \"en\" for the English)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"401c2488-760f-4374-930e-d2326fc4e632"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Now we can use \"from_json\" to parse out the message and provide schema\n\nfrom pyspark.sql.functions import from_json\n\nwiki_json_df = (wiki_payload_df\n                .select(from_json(\"value\", schema).alias(\"json\"))\n               )\n\ndisplay(wiki_json_df,streamName=\"wiki_json_stream\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17528c23-8656-4562-9fc7-11b2ba6eb7af"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# let's get only the anonymous article edits which have a location\n\nwiki_anon_df = (wiki_json_df\n  .select(col(\"json.wikipedia\").alias(\"wikipedia\"),      # Promoting from sub-field to column\n          col(\"json.isAnonymous\").alias(\"isAnonymous\"),  \n          col(\"json.namespace\").alias(\"namespace\"),\n          col(\"json.page\").alias(\"page\"),\n          col(\"json.pageURL\").alias(\"pageURL\"),\n          col(\"json.geocoding\").alias(\"geocoding\"),\n          col(\"json.user\").alias(\"user\"),\n          col(\"json.timestamp\").cast(\"timestamp\"))       # Promoting and converting to a timestamp\n  .filter(col(\"namespace\") == \"article\")                 # Limit result to just articles\n  .filter((col(\"geocoding.countryCode3\").isNotNull()))        # We only want results that are geocoded\n)\n\ndisplay(wiki_anon_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0e5ba03-bb1c-4e66-8b62-6a480842a34d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["wiki_loc_df = (wiki_anon_df\n  .groupBy(\"geocoding.countryCode3\") # Aggregate by country (code)\n  .count()                           # Produce a count of each aggregate\n)\ndisplay(wiki_loc_df, streamName = \"location_stream\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03212514-51fb-4e54-b863-fd6077f68b87"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["(wiki_loc_df\n .writeStream\n .format(\"delta\")\n .outputMode(\"complete\") # complete overwrite on every trigger\n .option(\"checkpointLocation\", \"/tmp/wikiloc/checkpoint\")\n .table(\"wikiloc\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6d663e4-1bec-4e57-bca8-510795abca3a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(spark.table(\"wikiloc\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7de09500-6110-4554-a1ec-898e76304986"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Time windows"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"475b45b8-9b3d-412e-9da5-0291489a5214"}}},{"cell_type":"code","source":["# commonly, you might not want an aggregation of a stream's whole history.\n# for this purpose, let's import window from functions - NB this is \"time windows\" not \"SQL-like window function\"\n\nfrom pyspark.sql.functions import window\n\nwiki_loc_window_df = (wiki_anon_df\n  .groupBy(\"geocoding.countryCode3\", window(\"timestamp\", \"5 minute\")) # Aggregate by country, every 5 minute block. This is a \"tumbling window\"\n  .count()\n)\ndisplay(wiki_loc_window_df, streamName = \"time_window_stream\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d432333a-1335-4fc4-8b5d-1e79f16ebbf9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# if we want to keep always the latest time window then we can use sliding windows\n\nwiki_loc_window_slide_df = (wiki_anon_df\n  .groupBy(\"geocoding.countryCode3\", window(\"timestamp\", \"5 minute\", \"1 minute\")) # Aggregate by country, 5 minute block sliding by 1 minute. This is a \"sliding window\"\n  .count()\n)\ndisplay(wiki_loc_window_df, streamName = \"time_window_slide_stream\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e86c785c-51ac-4780-be1e-184e01fee543"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# let's try again with a different, simpler dataset\n\n# %fs head dbfs:/mnt/training/sensor-data/accelerometer/time-series-stream.json/file-0.json\n\ninput_path = \"dbfs:/mnt/training/sensor-data/accelerometer/time-series-stream.json/\"\n\njson_schema = \"time timestamp, action string\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acac1e02-d555-4fdc-aa57-0eb83b64c55d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Let's create a dataframe and apply some transformations and aggregation\n\nfrom pyspark.sql.functions import window, col\n\ninput_df = (spark\n  .readStream                                 \n  .schema(json_schema)                       \n  .option(\"maxFilesPerTrigger\", 1)            \n  .json(input_path)                           \n)\n\ncounts_df = (input_df\n  .groupBy(col(\"action\"),                     # Aggregate by action\n           window(col(\"time\"), \"1 hour\"))     # and by a 1 hour window\n  .count()                                    # Count the actions\n  .select(col(\"window.start\").alias(\"start\"), \n          col(\"count\"),                       \n          col(\"action\"))                      \n  .orderBy(col(\"start\"))                      # Sort by the start time\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5f36054-4522-42b8-97aa-7fabeede5da0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# displaying the output\n\ncounts_stream = \"counts_stream\"\ndisplay(counts_df, streamName = counts_stream)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aea650a6-ed4e-44a2-8eb7-65d73326a046"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# observe the 200 partitions (Spark default) in the above query \n\nspark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism) # reduce parallelism for aggregations and joins, set it to amount of cores. Faster processing\n\ndisplay(counts_df, streamName = counts_stream)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cc3b01d-587d-4fde-93e5-3ce540980f97"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Watermarking"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e200d92c-a0ce-4b7c-be79-3658e2e56dd5"}}},{"cell_type":"code","source":["# in actual use cases, the queries above would keep running for a very long time and the amount of windows would grow indefinitely\n# keeping track of all the states puts pressure on memory\n# also it may often be irrelevant if delayed data updates our figures\n\nwatermarked_stream = \"watermarked_stream\"\n\nwatermarked_df = (input_df\n  .withWatermark(\"time\", \"2 hours\")             # Specify a 2-hour watermark\n  .groupBy(col(\"action\"),                       # Aggregate by action...\n           window(col(\"time\"), \"1 hour\"))       # ...then by a 1 hour window\n  .count()                                      # For each aggregate, produce a count\n  .select(col(\"window.start\").alias(\"start\"),   # Elevate field to column\n          col(\"count\"),                         # Include count\n          col(\"action\"))                        # Include action\n  .orderBy(col(\"start\"))                        # Sort by the start time\n)\ndisplay(watermarked_df, streamName = watermarked_stream) # Start the stream and display it\n\n# important note: watermarking guarantees that any event within the window gets in. It does not guarantee leaving anything out."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d968b6b-01a9-480a-8d63-b3e5c0ca159a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Let's import another dataset. Let's say we are interested in hourly monitoring of incoming traffic to our website\n\nschema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n\nhourlyEventsPath = \"/mnt/training/ecommerce/events/events-2020-07-03.json\"\n\nwebsite_df = (spark.readStream\n  .schema(schema)\n  .option(\"maxFilesPerTrigger\", 1)\n  .json(hourlyEventsPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf4aa920-978f-4e07-ab18-efca64d10f51"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# this dataframe does not have a proper timestamp column. So we need to create one and use it for watermarking\n\nfrom pyspark.sql.functions import col\n\nevents_df = (website_df\n             .withColumn(\"createdAt\", (col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n             .withWatermark(\"createdAt\", \"2 hours\")\n)             "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81d948ed-3a26-45e7-8e92-836cc07ae9c0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# now we can do an aggregation\nfrom pyspark.sql.functions import approx_count_distinct, hour, window\n\ntraffic_df = (events_df\n             .groupBy(\"traffic_source\"\n                      , window(col(\"createdAt\"), \"1 hour\"))\n             .agg(approx_count_distinct(\"user_id\").alias(\"active_users\"))\n             .select(col(\"traffic_source\")\n                     , col(\"active_users\")\n                     , hour(col(\"window.start\")).alias(\"hour\"))\n             .sort(\"hour\")\n)\n\ndisplay(traffic_df, streamName=\"hourly_traffic_p\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb7b62a6-8a6d-41bc-a226-5c5d7cf40943"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Joining streams"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"307411ab-59c4-4026-b6d8-65b745ca1d28"}}},{"cell_type":"code","source":["# let's load in users dataset\n\nusers_df = spark.read.parquet(\"/mnt/training/ecommerce/users/users.parquet/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d2d54db-d035-4163-a9af-679884a3edb1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# join works same way as with regular dataframes.\n# note: this is streaming<->static join\n\njoined_df = (events_df\n            .join(users_df, \"user_id\")\n            )\n\ndisplay(joined_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"600645dc-8cb3-47ba-8d28-bc6330cefdb0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# example on subset of dataframe\n\njoined_limit_df = (events_df\n            .join(users_df.limit(500000), \"user_id\")\n            )\n\ndisplay(joined_limit_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d20578d4-86b4-4ef7-ac6e-818937061eea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# let's read in users dataframe as a stream\n\n# since we have created the dataframe from this data, we can cheat on getting the schema. Possible in development/debugging, not possible or recommended in production\nusers_schema = users_df.schema\n\nusers_stream_df = (spark\n                   .readStream\n                   .format(\"parquet\")\n                   .schema(users_schema)\n                   .option(\"maxFilesPerTrigger\", 1)\n                   .parquet(\"/mnt/training/ecommerce/users/users.parquet/\")\n                  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da8f8dbd-c601-4ed6-b96a-0af6abbabf36"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# let's do a stream to stream join\n\njoined_streams_df = (events_df\n            .join(users_stream_df, \"user_id\")\n            )\n\ndisplay(joined_streams_df, processingTime=\"10 seconds\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eab5049a-272a-468b-a80a-0b25830b93fe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["for stream in spark.streams.active:\n  stream.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6ddcecc-b7c5-427e-bbd7-fea0f4886160"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Optional feedback\n\nhttps://tinyurl.com/structstream"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f90e7128-421a-4e79-b4bd-ab26dd24ed08"}}},{"cell_type":"markdown","source":["### Further reading\n\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html  \nhttps://docs.databricks.com/spark/latest/structured-streaming/index.html"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2ffb003-abb3-4689-9ad0-8564ac25e1cb"}}},{"cell_type":"markdown","source":["### Task 1\n\nCreate a streaming dataframe from the data in the following path:  \n/mnt/training/asa/flights/2007-01-stream.parquet/\n\nThe schema should contain\n* DepartureAt (timestamp)\n* UniqueCarrier (string)\n\nProcess only 1 file per trigger.  \n\nAggregate the data by count, using non-overlapping 30 minute windows.  \nIgnore any data that is older than 6 hours.\n\nThe output should have 3 columns: startTime (window start time), UniqueCarrier, count.  \nThe output should be sorted ascending by startTime.\n\nDisplay the output, firing the trigger every 5 seconds.\n\nOnce the stream has produced some output, call the stream shutdown function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b127445-2b2f-49fb-84fe-fc9063f2450f"}}},{"cell_type":"code","source":["# Your answer\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4204d127-eb1f-4c1f-be71-dd4c638935f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Task 2\n\nUsing the same server as above, subscribe to the Kafka topic \"tweets\".\n\nCreate the following schema for the JSON payload:  \nroot  \n |-- hashTags: array (nullable = true)  \n |    |-- element: string (containsNull = true)  \n |-- text: string (nullable = true)  \n |-- id: long (nullable = true)  \n |-- createdAt: long (nullable = true)  \n |-- retweetCount: integer (nullable = true)  \n |-- favoriteCount: integer (nullable = true)  \n |-- user: string (nullable = true)  \n |-- userScreenName: string (nullable = true)  \n \nParse out the payload and aggregate by count on the _size_ of the hashTags array. Name the column \"amountOfHashtags\".  \n\nExample output:</br>\n<table>\n  <tr>\n    <th>amountOfHashtags</th>\n    <th>count</th>\n  </tr>\n  <tr>\n    <td>0</td>\n    <td>500</td>\n  </tr>\n  <tr>\n    <td>1</td>\n    <td>120</td>\n  </tr>\n  <tr>\n    <td>3</td>\n    <td>15</td>\n  </tr>\n</table>\n\nWrite the output to a delta table called \"hashtags\", completely overwriting the result on every trigger.\n\nMake sure to display what data the table contains. Then shutdown the stream."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d97f4aa-93ca-4793-875a-a96486416145"}}},{"cell_type":"code","source":["# Your answer\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"145af781-e1f0-4c70-83bf-48832653287a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Practice session - Structured Streaming (cont)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1637900618322378}},"nbformat":4,"nbformat_minor":0}
