{"cells":[{"cell_type":"markdown","source":["### Task\n\nUsing the same server as above, subscribe to the Kafka topic \"tweets\".\n\nCreate the following schema for the JSON payload:  \nroot  \n |-- hashTags: array (nullable = true)  \n |    |-- element: string (containsNull = true)  \n |-- text: string (nullable = true)  \n |-- id: long (nullable = true)  \n |-- createdAt: long (nullable = true)  \n |-- retweetCount: integer (nullable = true)  \n |-- favoriteCount: integer (nullable = true)  \n |-- user: string (nullable = true)  \n |-- userScreenName: string (nullable = true)  \n \nParse out the payload and aggregate by count on the _size_ of the hashTags array. Name the column \"amountOfHashtags\".  \n\nExample output:</br>\n<table>\n  <tr>\n    <th>amountOfHashtags</th>\n    <th>count</th>\n  </tr>\n  <tr>\n    <td>0</td>\n    <td>500</td>\n  </tr>\n  <tr>\n    <td>1</td>\n    <td>120</td>\n  </tr>\n  <tr>\n    <td>3</td>\n    <td>15</td>\n  </tr>\n</table>\n\nWrite the output to a delta table called \"hashtags\", completely overwriting the result on every trigger.\n\nMake sure to display what data the table contains. Then shutdown the stream."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2393a9af-b9de-4c1b-b48a-d93e78e7b92d"}}},{"cell_type":"code","source":["# ANSWER\n\nfrom pyspark.sql.types import * \nimport pyspark.sql.functions as F\n\nkafka_server = \"server1.databricks.training:9092\"   # US (Oregon)\n\ntweet_df = (spark.readStream                        # Get the DataStreamReader\n  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n  .option(\"subscribe\", \"tweets\")                       # Subscribe to the \"en\" Kafka topic - edits of English wikipedia pages\n  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n  .load()                                          # Load the DataFrame\n)\n\n\ntweet_schema = StructType([\n  StructField(\"hashTags\", ArrayType(StringType(), True), True),\n  StructField(\"text\", StringType(), True),\n  StructField(\"id\", LongType(), True),\n  StructField(\"createdAt\", LongType(), True),\n  StructField(\"retweetCount\", IntegerType(), True),\n  StructField(\"favoriteCount\", IntegerType(), True),\n  StructField(\"user\", StringType(), True),\n  StructField(\"userScreenName\", StringType(), True)\n])\n\n\ntweet_payload_df = (tweet_df\n                  .select(F.col(\"value\").cast(StringType()))\n                  )\n\ntweet_json_df = (tweet_payload_df\n                .select(F.from_json(\"value\", tweet_schema).alias(\"json\"))\n               )\n\ncounts_df = (tweet_json_df\n             .select(F.size(\"json.hashTags\").alias(\"hashtags\"))\n             .groupBy(\"hashtags\")\n             .count()\n            )\n\n(counts_df\n .writeStream\n .format(\"delta\")\n .outputMode(\"complete\")\n .option(\"checkpointLocation\", \"/tmp/countstest/checkpointing\")\n .table(\"hashtag_counts\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc70b838-0491-43f6-bf0d-578c8799cadb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(spark.table(\"hashtag_counts\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7485d64-d350-4c98-9f15-8eec5fe97039"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["for stream in spark.streams.active:\n  stream.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7bda244a-0c9a-4e66-b16b-2f698edb32fb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Practice session - Delta Lake, Structured Streaming (answers)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2212245589459853}},"nbformat":4,"nbformat_minor":0}
