{"cells":[{"cell_type":"markdown","source":["### Second session on Dataframes + introduction to Structured Streaming\n\n* Dataframes\n    * Custom schemas\n    * Pivot\n    * Window functions\n    * Delta Lake"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b131d293-c37a-4753-95e3-85a755db6aaa"}}},{"cell_type":"markdown","source":["#### Custom schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5787966d-8ae7-4a2c-a7f2-2199099809f6"}}},{"cell_type":"code","source":["# Let's import a JSON dataset and have a look at the file\n\nimport requests\nr = requests.get(\"https://think.cs.vt.edu/corgis/datasets/json/airlines/airlines.json\")\n\nprint(r.json()[:2])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a605b371-d189-4bc6-9eb6-7bed18b9b2d4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# creating a dataframe from this JSON string (Python dictionary) gives us many nested MapType columns\nfrom pyspark.sql import Row\n\n#airlines_df = spark.createDataFrame(r.json()) # another way of creating df from dict. Inferring schema from Python dict is deprecated, but this may still work\n#airlines_df = spark.createDataFrame(Row(**x) for x in r.json()) # unpacking each JSON element into a spark Row element\n\ndisplay(airlines_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7c22a9e-a4da-4b73-8ab2-128d0d9345f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# let's test some functions on map columns\n\nfrom pyspark.sql.functions import *\n\ndisplay(airlines_df\n        .select(explode(\"Airport\") # we can try explode - but this does not make sense for this kind of data\n                #,col(\"Airport\").getItem(\"Code\").alias(\"AirportCode\") # we can use getItem to fetch values per key - good for small map columns, not good for large/nested map columns\n                #,col(\"Airport\").getItem(\"Name\").alias(\"AirportName\")\n                #,col(\"Airport.Code\"),col(\"Airport.Name\") # alternative, easier to call but may confuse as it looks like struct. But Airport.* does not work for map\n                #,map_keys(\"Airport\") # array of keys\n                #,map_values(\"Airport\") # array of values\n                ,\"*\")\n        )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d10699ed-12db-4f47-a18b-57aefe56caa1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# usually, a struct type of column is more useful and easier to access\n# in this case, we need to custom define a schema when reading in the DataFrame.\n# this is also recommended for real-life data pipelines, especially in case of large amount of small data files\n# potential down-side: missing schema evolution\n\nfrom pyspark.sql.types import *\n\n# The outer part needs to be a StructType\n# A StructType needs to consist of StructFields\n# StructFields have 3 parameters: name, type, nullable\n\n# Note: you can remove or add parts of schema\n# Note2: name has to match the key/column name in the dataset.\n\nairport_schema = StructType([\n  StructField(\"Airport\",StructType([\n    StructField(\"Code\", StringType(), True),\n    StructField(\"Name\", StringType(), True)\n  ]),True), \n  StructField(\"Statistics\",StructType([\n    StructField(\"Carriers\", StructType([\n      StructField(\"Names\", StringType(), True),\n      StructField(\"Total\", IntegerType(), True)\n    ]), True),\n    StructField(\"Minutes Delayed\", StructType([\n      StructField(\"Late Aircraft\", LongType(), True),\n      StructField(\"National Aviation System\", LongType(), True),\n      StructField(\"Weather\", LongType(), True),\n      StructField(\"Carrier\", LongType(), True),\n      StructField(\"Security\", LongType(), True),\n      StructField(\"Total\", LongType(), True)\n    ]), True),\n    StructField(\"Flights\", StructType([\n      StructField(\"Delayed\", LongType(), True),\n      StructField(\"Diverted\", LongType(), True),\n      StructField(\"Cancelled\", LongType(), True),\n      StructField(\"On Time\", LongType(), True),\n      StructField(\"Total\", LongType(), True)\n    ]), True),\n    StructField(\"# of Delays\", StructType([\n      StructField(\"Late Aircraft\", LongType(), True),\n      StructField(\"National Aviation System\", LongType(), True),\n      StructField(\"Weather\", LongType(), True),\n      StructField(\"Carrier\", LongType(), True),\n      StructField(\"Security\", LongType(), True)\n    ]), True)\n  ]),True),\n  StructField(\"Time\",StructType([\n    StructField(\"Label\", StringType(), True),\n    StructField(\"Month\", IntegerType(), True),\n    StructField(\"Year\", IntegerType(), True),\n    StructField(\"Month Name\", StringType(), True)\n  ]),True)\n  #StructField(\"MyNullTimestamp\", TimestampType(), True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2088142-318a-4ccf-a9d6-9c81106753e2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# we can now provide this schema as input in our dataframe creation\n\nairport_schema_df = spark.createDataFrame(r.json(), schema=airport_schema) #spark is not inferring schema, so inputting dictionary works fine\ndisplay(airport_schema_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43f3c3f5-013c-4972-93c8-90f4b1c834d0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# another way to create a schema is using StructType's \"add\" method\n\nairport_add_schema = (StructType()\n                      .add(\"Airport\", StructType()\n                          .add(\"Code\", StringType())\n                          .add(\"Name\", StringType())\n                          )\n                      .add(\"Time\", StructType()\n                          .add(\"Month\",IntegerType())\n                          .add(\"Year\",IntegerType()))\n                     )\n\nairport_add_schema_df = spark.createDataFrame(r.json(), schema=airport_add_schema) #spark is not inferring schema, so inputting dictionary works fine\ndisplay(airport_add_schema_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c166420d-84e2-4687-84e7-7e80bf22b509"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# third method, raw string\nairport_string_schema = \"Airport STRUCT<Code: STRING, Name: STRING>, Time STRUCT<Month: INTEGER, Year: INTEGER, Date: INTEGER>\" # date will be null\n\nairport_string_schema_df = spark.createDataFrame(r.json(), schema=airport_string_schema) #spark is not inferring schema, so inputting dictionary works fine\ndisplay(airport_string_schema_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3de399ef-7573-4125-909b-8d9bc8d4a5d6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# It is now much easier to navigate and manipulate the fields.\n\ndisplay(airport_schema_df\n        .select(\"Airport.*\"\n               ,\"*\"\n               )\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e062ad80-2131-4259-91de-fcd6fe4def36"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Pivot"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61fef9e0-3794-47e6-8edf-15c469ea99db"}}},{"cell_type":"code","source":["# pivot table - summarizing a more extensive table\n# let's first load in a dataset\n\nairbnb_df = spark.read.parquet(\"mnt/training/airbnb/amsterdam-listings/amsterdam-listings-2018-12-06.parquet/*\")\ndisplay(airbnb_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d63649e2-8a56-4082-bf9f-24e716f4eeee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# the dataset has many columns. Let's say we are interested in the average prices per city and neighbourhood.\n# we are also interested in the size of the place - how many people it accommodates\n\ndisplay(airbnb_df.select(\"city\"\n                       , \"neighbourhood\"\n                       , \"accommodates\"\n                       , \"price\")\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e43851a6-f96a-4d36-9754-1e052a2e7dff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import desc\n\ndisplay(airbnb_df\n        .select(\"city\", \"neighbourhood\", \"accommodates\", \"price\")\n        .groupby(\"city\", \"neighbourhood\") # \"row\" \n        .pivot(\"accommodates\") # columns\n        .mean(\"price\") # data / values     # possible options: mean, sum, min, max, count\n        #.na.fill(0) # for filling out null values. 0 for counts/sums\n        #.orderBy(desc(\"2\")) # for ordering\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30cf918b-6f60-451a-9d91-af362dfa258c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# another pivot example\n\ndf_wiki = spark.read.parquet(\"/mnt/training/wikipedia/pageviews/pageviews_by_second.parquet/*\")\n\ndisplay(df_wiki)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eaf9a0dc-15d1-4e2b-8010-6200c806a786"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(df_wiki\n        .selectExpr(\"cast(timestamp as date) as date\"\n                   ,\"hour(timestamp) as hour\"\n                   ,\"site\"\n                   ,\"requests\")\n        .groupBy(\"hour\") #\"date\"\n        .pivot(\"site\")\n        .sum(\"requests\")\n        .orderBy(\"hour\") #\"date\" \n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74b9a8b0-01bb-4faf-a259-480d6ef292c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### SQL window functions\n\n_Note: a \"window\" can be many different things. Here we talk about classical SQL window functions_"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d241577-126c-4ddb-b0a9-f1b85010ed32"}}},{"cell_type":"code","source":["# let's load in a new dataset and have a look at it\nhealthcare_df = spark.read.parquet(\"/mnt/training/healthcare/tracker/health_profile_data.snappy.parquet\")\ndisplay(healthcare_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1988f5d-bb39-43be-83e3-0ab05b99505c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# we need to import the Window API and instantiate a Window specification object\n# let's also import pyspark.sql.functions for using aggregations and functions  \n\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import Window\n\nwindow_spec = Window.partitionBy(\"_id\").orderBy(\"dte\") \n\ndisplay(healthcare_df\n       .withColumn(\"row_num\", row_number().over(window_spec)) # similarly can use rank and dense_rank\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"226bb17c-c10c-4340-ad51-3700ffeae812"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# you can use multiple window specs in parallel\n  \nwindow_spec_by_hr = Window.partitionBy(\"_id\").orderBy(\"resting_heartrate\")\n\ndisplay(healthcare_df\n       .withColumn(\"row_num\", row_number().over(window_spec)) # similarly can use rank and dense_rank\n       .withColumn(\"rank_num\", rank().over(window_spec_by_hr))\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"442522a1-4b5c-41e1-b4dc-3b1feb95d374"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# use lag / lead for viewing \"back\" or \"ahead\" within a partition's rows\n\ndisplay(healthcare_df\n       .withColumn(\"lag\", lag(\"resting_heartrate\").over(window_spec)) # use for getting previous/next value in partition. \n       .withColumn(\"lead\", lead(\"resting_heartrate\", 5, 0).over(window_spec)) # Can define offset and default value\n       #.withColumn(\"diffToPrev\",expr(\"resting_heartrate - lag\")) # useful for getting the increase/decrease\n       )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cad161ca-e70f-4ac5-8bca-b01a2e44496f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# rolling windows - useful for moving averages, rolling total, etc\n\nwindow_spec_rolling = Window.partitionBy(\"_id\").orderBy(\"dte\").rowsBetween(Window.unboundedPreceding, Window.currentRow) # rolling aggregations - everything up to current row\n#window_spec_rolling_last_week = Window.partitionBy(\"_id\").orderBy(\"dte\").rowsBetween(-6, Window.currentRow) # rolling aggregations, use negative integer for previous rows\n#window_spec_rolling_plusmin2 = Window.partitionBy(\"_id\").orderBy(\"dte\").rowsBetween(-2, 2) # rolling aggregations, use positive integer for next rows\n\ndisplay(healthcare_df\n       .withColumn(\"row_num\", row_number().over(window_spec)) \n       .withColumn(\"rolling_avg\", avg(\"resting_heartrate\").over(window_spec_rolling)) # using aggregations with window functions\n       #.withColumn(\"row_num_sum\", sum(\"row_num\").over(window_spec_rolling_plusmin2))\n       #.withColumn(\"max_BMI_3\", max(\"BMI\").over(window_spec_rolling_plusmin2))\n       )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7792582-a539-4c3f-a0b8-2b3ce9c0fff8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Delta Lake"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"655b6c16-409e-4112-a9be-bbc50fc5b040"}}},{"cell_type":"code","source":["# let's load in a small df for demonstration purposes\n\niso_df = (spark.read\n          .option(\"header\",\"true\")\n          .option(\"inferSchema\",\"true\")\n          .csv(\"/mnt/training/countries/ISOCountryCodes/ISOCountryLookup.csv\")\n         )\ndisplay(iso_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d1a2200-97e5-4950-b271-f4cc9e589c6c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# General issue with data lakes / Hive tables / HDFS storage\n# Hard to do SQL / Data Warehouse-like updates\n# No ACID compliance (Hive has now in newer versions, but not inherently compatible with Spark)\n\n# Delta Lake = open source\n# ACID, time-travel, optimized performance, ...\n\niso_df.write.mode(\"overwrite\").saveAsTable(\"hive_iso_t\")\niso_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_iso_t\") #format \"delta\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ceebbea-7f30-4259-aa9b-c6cb1dcfc4d4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(spark.table(\"hive_iso_t\"))\n#display(spark.table(\"delta_iso_t\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20fe7ef3-4fca-455e-a74d-09d84fa8adf8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# updating using spark sql statements\n#spark.sql(\"UPDATE hive_iso_t SET independentTerritory = 'Yes' WHERE EnglishShortName = 'Antarctica'\") # fails, update not supported\n#spark.sql(\"UPDATE delta_iso_t SET independentTerritory = 'Yes' WHERE EnglishShortName = 'Antarctica'\") # OK\n\ndisplay(spark.table(\"delta_iso_t\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ed135f4-1bcf-4041-88a8-aa5c278b9307"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# another way is to use delta API, useful for more advanced usecases and simpler programmability\n\nfrom delta.tables import *\n\ndelta_table = DeltaTable.forName(spark, \"delta_iso_t\") \n\ndelta_table.update(\n  condition = \"EnglishShortName = 'Greenland'\",\n  set = { \"independentTerritory\": \"'Yes'\"}\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28aa1aac-b11f-4572-b366-85b020cf5807"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# view history of table\n\ndisplay(delta_table.history()) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32c348dd-9a10-42ba-add0-dee1b14fde8f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# creating dataframe from previous state\n\ndisplay(spark.sql(\"DESCRIBE EXTENDED delta_iso_t\")) # getting the table path\n\n#timestamp_df = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2021-03-20 09:55:00\").load(\"/user/hive/warehouse/delta_iso_t\")\n#display(timestamp_df)\n#version_df = spark.read.format(\"delta\").option(\"versionAsOf\", 4).load(\"/user/hive/warehouse/delta_iso_t\")\n#display(version_df)\n\n# restoring table to previous state\n\n#delta_table.restoreToTimestamp('2021-03-20 10:20') # restore to a specific timestamp\n#delta_table.restoreToVersion(0) # restore table to (oldest) version\n#display(spark.table(\"delta_iso_t\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19772ea0-d0ed-4de9-a228-449fb6153398"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Structured streaming\n\nCommon input/output:\n* Input sources\n  * Kafka (and other distributed commit logs)\n  * Files on a distributed system\n  * TCP-IP sockets _note: not fault-tolerant_\n* Output (sinks)\n  * Kafka (etc)\n  * File formats\n  * Spark tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d095919-1574-47e6-bd89-58298304e99f"}}},{"cell_type":"code","source":["# let's start by looking at reading and writing streams to files\n# we need a schema when reading streams\n\nevents_schema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n\nstreaming_events_df = (spark.readStream\n  .schema(events_schema)\n  .option(\"maxFilesPerTrigger\", 1) # used for example purposes, reads in 1 file per trigger\n  .parquet(\"/mnt/training/ecommerce/events/events.parquet\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a000241-a9e5-4dbd-9e47-e3b4db029f33"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# you can check if a dataframe has streaming sources\n# this means some functions are unavailable (eg count)\n\nstreaming_events_df.isStreaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd55451d-fefc-4712-bffd-a92391082893"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# let's create a new dataframe and write it into a file\n\nemail_df = (streaming_events_df\n            .filter(col(\"traffic_source\") == \"email\")\n            .withColumn(\"mobile\", col(\"device\").isin([\"iOS\", \"Android\"]))\n            .select(\"user_id\", \"event_timestamp\", \"mobile\")\n           )\n\ncheckpoint_path = \"/tmp/email_t2/checkpoint\" #\"tmp/email_traffic/checkpoint\"\noutput_path = \"/tmp/email_t2/output\"\n\ndevices_query = (email_df.writeStream\n  .outputMode(\"append\") # append = only new rows, complete = all rows written on every update, update = only updated rows (used in aggregations, otherwise same as append)\n  .format(\"parquet\")\n  .queryName(\"email_traffic_p\") # optional name\n  .trigger(processingTime=\"10 second\") # how often data is fetched from source\n  .option(\"checkpointLocation\", checkpoint_path) # used for fault-tolerance. Note: every query needs to have a unique check point location\n  .start(output_path) # location where the file will be written\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"609a521d-6546-4d7b-80c3-1ead4eff0aea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs ls tmp/email_t2/output"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19b7ef6d-ab60-4a04-ac17-f36ea3635c2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# monitor the query\n\ndevices_query.id # unique per query, persisted when restarted from checkpoint \n#devices_query.name\n#devices_query.status # isDataAvailable = new data available, isTriggerActive = trigger actively firing\n#devices_query.awaitTermination(5) # timeout in seconds, can use to keep cluster awake, also useful for seeing if stream was quit or got an exception\n#devices_query.stop() # note: for streaming data, cluster will keep awake (processing ongoing)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16f07a81-32c1-401c-aec4-ce4051aa94bc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# fetching from TCP\n\nlogs_df = (spark.readStream\n    .format(\"socket\")\n    .option(\"host\", \"server1.databricks.training\")\n    .option(\"port\", 9001)\n    .load()\n)\n\ndisplay(logs_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73c228bd-d68a-49a0-9262-9a9b8a858925"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# we can look at a count by using the agg function\n\nrunning_count_df = logs_df.agg(count(\"*\"))\n\ndisplay(running_count_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbc4490f-5e71-44a3-84ef-97db2f3c21be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# let's see how comfortable it is manipulating the streaming dataframe\n# we want to get all rows where there is an error\n\n# Start by parsing out the timestamp and the log data\nclean_df = (logs_df\n            .withColumn(\"ts_string\", col(\"value\").substr(2, 23))\n            .withColumn(\"epoch\", unix_timestamp(\"ts_string\", \"yyyy/MM/dd HH:mm:ss.SSS\"))\n            .withColumn(\"capturedAt\", col(\"epoch\").cast(\"timestamp\"))\n            .withColumn(\"logData\", regexp_extract(\"value\", \"\"\"^.*\\]\\s+(.*)$\"\"\", 1))\n           )\n\n# Keep only the columns we want and then filter the data\nerrors_df = (clean_df\n             .select(\"capturedAt\", \"logData\")\n             .filter(col(\"value\").like(\"% (ERROR) %\"))\n            )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"557baca1-46b7-4f57-a93b-6620a84ffa65"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# let's write the errors dataframe into a delta table\n\n(errors_df.writeStream\n  .format(\"delta\")\n  .outputMode(\"append\")\n  .option(\"checkpointLocation\", \"/tmp/events_stream/_checkpoints/\")\n  .table(\"errors_streaming\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ffc6e36-a824-4911-a47c-9a8487cd0d02"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# we can view snapshots from this table\n\ndisplay(spark.table(\"errors_streaming\")\n        .sort(desc(\"capturedAt\"))\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6bc9fb39-e294-4dcc-8d52-c81f959dc9e8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# we can also check the count of this table increasing\n\nspark.sql(\"SELECT COUNT(*) FROM errors_streaming\").first()[0]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f362145d-79e4-4ebc-8162-ff7e4e87d776"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# since it was a delta table, we can look at the history\n\nerrors_table = DeltaTable.forName(spark, \"errors_streaming\")\n\ndisplay(errors_table.history())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd5f08ec-9d23-412b-9ba2-a2a34091ceb0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# use spark.streams.active to loop over all active streams\n# remember to stop streams if not working on them anymore\n\nfor stream in spark.streams.active:\n  print(stream.name)\n  #stream.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae5d3804-072b-4846-94b5-f0c99699d65a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Further reading\n\n* Spark SQL Window functions\n  * https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#window\n* Delta Lake: \n  * https://delta.io/\n* Structured Streaming: \n  * https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss.html\n  * https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html\n  * http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b4d9fd9-1d6b-4884-ac51-afe9c2b8b32b"}}},{"cell_type":"markdown","source":["### Optional anonymous feedback:\n\nhttp://tinyurl.com/dfandssintro"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"727c8cc9-bb53-4f39-acf1-0fd3bc9760e0"}}},{"cell_type":"markdown","source":["### Tasks for session 3"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6c65d7a-9c3e-4c18-b886-140896a5e76a"}}},{"cell_type":"markdown","source":["#### Task 1\n\nEmployees' dataset: \"/mnt/training/manufacturing-org/employees/employees.csv\"\n\nUsing window functions, find the **employees** who have worked in a specific **department** the *longest* and *shortest* time.\n\nResulting dataframe should have 3 columns: employee_name, department, employment_duration\n\nEmployment_duration should have 2 possible values: \n* **longest**\n  * The employee has worked in the department for the longest time. Based on column _active_record_start_\n* **shortest**\n  * The employee has worked in the department for the shortest time. Based on column _active_record_start_\n\nResulting dataframe should have total 6 rows.\n\nExample df.take(3):</br>\n<table>\n  <tr>\n    <th>employee_name</th>\n    <th>department</th>\n    <th>employment_duration</th>\n  </tr>\n  <tr>\n    <td>CISNEROS JR, HERBERT</td>\n    <td>OFFICE</td>\n    <td>shortest</td>\n  </tr>\n  <tr>\n    <td>CRAVEN, KEVIN J</td>\n    <td>OFFICE</td>\n    <td>longest</td>\n  </tr>\n  <tr>\n    <td>WRIGHT, RONALD G</td>\n    <td>PRODUCTION</td>\n    <td>shortest</td>\n  </tr>\n</table>\n\nNote: highest points are awarded to solutions where dataframe has least transformations.</br>\nShould be doable with 2 window functions and 2 transformations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75173e12-22f5-4848-b44f-e62e34597d05"}}},{"cell_type":"code","source":["# your answer"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0afee002-3810-4c66-a8f7-cbc3113a32ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Task 2\n\nCreate a schema and a streaming dataframe for the JSON files in the following path:  \n\"/mnt/training/gaming_data/mobile_streaming_events_b/\"\n  \n  \nUse the following as basis for creating your schema:  \n |-- eventName: string (nullable = true)  \n |-- eventParams: struct (nullable = true)  \n |    |-- amount: double (nullable = true)  \n |    |-- app_name: string (nullable = true)  \n |    |-- app_version: string (nullable = true)  \n |    |-- client_event_time: string (nullable = true)  \n |    |-- device_id: string (nullable = true)  \n |    |-- game_keyword: string (nullable = true)  \n |    |-- platform: string (nullable = true)  \n |    |-- scoreAdjustment: long (nullable = true)  \n  \nRead in 2 files per trigger.\n  \nCreate a new modified dataframe:\n* keep only rows where eventName is \"scoreAdjustment\"\n* select the *game_keyword*, *platform* and *scoreAdjustment* columns from the eventParams struct.  \n* set trigger to run every 5 seconds.\n\nWrite the datastream to a delta table called score_adjustments.  \nCheck to make sure that the table has some data - this should also be visible in the cell results.  \nThen stop the datastream."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f980944-6bcd-4fdc-8081-f5efed6f66dc"}}},{"cell_type":"code","source":["# your answer"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd9cca6f-5579-4372-8a37-3dfe1ff96d2e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Practice session - Dataframes (cont), Structured Streaming","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1794969491294066}},"nbformat":4,"nbformat_minor":0}
